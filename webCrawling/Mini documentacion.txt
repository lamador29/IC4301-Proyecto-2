->links, web scraping recursivo en Wikipedia

El archivo contiene 2 variables al inicio, url donde se coloca desde donde se desea iniciar la busqueda y
depth que indica la profundidad de la recursion. Es muy poco aconsejable usar ninguna profundidad
mayor a 2 debido al estres que genera en recursos computacionales. Se estima que depth 5 deberia de
descargar a almenos la mitad de la base de datos de wikipedia, dependiendo de donde se comienza.

El script va recogiendo de manera recursiva todas las referencias en un articulo de wikipedia hacia otros
articulos de wikipedia, los cuales tendrán más referencias dentro que pueden usarse para seguir con el 
ciclo, esto se repite segun la profundidad que se desea. Despues de este proceso recursivo, el script pasa
a descargar y procesar los datos de manera concurrente con todos los procesadores en el dispositivo que
lo ejecuta, esto (y tambien los ciclos de recoger enlaces) se logra mediante la libreria de Joblib y su apartado
de multiprocesamiento. Eventualmente cuando los datos son procesados, se escriben tanto en formato CSV 
como JSON en el mismo directorio que el archivo ejecutado.